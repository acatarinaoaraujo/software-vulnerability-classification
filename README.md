## Software Vulnerability Classification using Large Language Models

## Table of Contents
- [Abstract](#abstract)
- [Research Questions](#research-questions)
- [Methodology](#methodology)
- [Conclusion](#conclusion)
- [Download Models](#download-models)
- [Repository Structure](#repository-structure)

## ABSTRACT
Software vulnerabilities are critical weaknesses in software systems that can compromise security. Our study utilizes the National Vulnerability Database (NVD) and the National Institute of Standards and Technology’s Vulnerability Description Ontology (VDO) framework to enhance the
identification and categorization of these vulnerabilities. By using Large Language Models (LLMs)
to analyze vulnerability descriptions, we cover 27 attributes across five domains. Our methodology
involves an in-depth analysis of NVD data and employs a variety of machine learning algorithms to
assess their classification capabilities. Among the LLMs, DistilBERT was the top performer with
an F1 score of 0.92, although BERT, XLNet, and DeBERTa V3 also showed strong performances
with F1 scores closely trailing at 0.91. Notably, these LLMs demonstrated comparable effectiveness
to traditional Decision Trees and Ensemble Learning models. The study also examines the effects
of dataset augmentation with recent data from 2021 to 2023, finding volatile impacts on model
performance and emphasizing the importance of rigorous quality control in data annotation.

## RESEARCH QUESTIONS

This paper explores the application of large language models (LLMs) to the task of software vul-
nerability classification. Our approach utilize various Transformer architectures including BERT,
DistilBERT, XLNet, DEBERTA V3, and LLAMA 2 to classify attribute characteristics based on
the Vulnerability Description Ontology (VDO)established by the National Institute of Standards
and Technology (NIST) Booth [4]. The study is structured around three primary research ques-
tions that aim to dissect the effectiveness, enhancement through data augmentation, and linguistic
features relevant to vulnerability classification using LLMs.
Research Questions:

### RQ1: How do large language models impact the effectiveness of software vulnerability classification?

This question seeks to evaluate the performance of LLMs in the software vulnerability classification domain. It encompasses two sub-questions:

- **RQ1.1: How do software vulnerability characterizations compare between large language models and conventional machine learning models in terms of F1 scores?**  
  We compare the effectiveness of LLMs with traditional machine learning models in classifying software vulnerabilities, focusing specifically on F1 scores as a measure of accuracy.

- **RQ1.2: Which models demonstrate the highest effectiveness in software vulnerability classification across various characterizations?**  
  This sub-question aims to identify which models—among a range of LLMs and conventional models—demonstrate the highest effectiveness across various vulnerability types.

### RQ2: Does the augmentation of datasets lead to enhanced performance in software vulnerability classification by large language models?

This question addresses whether extending the quantity and diversity of training data can lead to more robust models capable of understanding and categorizing a wider array of vulnerability types.

### RQ3: What are the prevalent N-grams associated with each software vulnerability characterization?

The third question shifts focus to the linguistic aspects, exploring the specific N-grams that are most prevalent and potentially indicative of different types of software vulnerabilities as classified by LLMs.

## METHODOLOGY
Our research introduces an approach that leverages natural language processing (NLP) and machine learning (ML) techniques to train classifiers capable of analyzing Common Vulnerabilities and Exposures (CVE) reports and inferring their associated Vulnerability Description Ontology (VDO) characteristics. We posit that the lexicon utilized within CVE descriptions contains valuable indicators of these characteristics, and through machine learning, classifiers can be trained to recognize and extract these indicators from historical data.

To validate our approach, we engaged in a multi-stage process:
- We created a labeled dataset through the manual curation of vulnerability descriptions corresponding to each of the 27 VDO characteristics. We also reused Okutan's dataset and merged it with ours.
- Transform-based classifiers were trained using this dataset. The primary goal is to predict the labels for text descriptions within each of the five distinct noun groups. These groups feature different labeling schemes:
  1. Multi-class classifications (Attack Theater, Context, Impact Method), where a text description contains a single label.
  2. Multi-label classifications (Logical Impact and Mitigation), where a text description contains more than one label.
- Finally, we computed a set of evaluation metrics to benchmark the classifiers' performance.


## CONCLUSION
Our study evaluated the impact of Large Language Models (LLMs) on software vulnerability classification, revealing that DistilBERT, BERT, XLNet, and DeBERTa V3 all demonstrated
strong performances. DistilBERT, in particular, achieved the highest F1 score at 0.92, illustrating
the high capability of LLMs in classifying software vulnerabilities effectively. Interestingly, while
LLMs generally outperformed the aggregate results from previous studies, traditional models such
as Decision Trees and Ensemble Learning displayed highly competitive and occasionally superior
performances in specific assessments.
Regarding dataset augmentation, incorporating recent data from 2021 to 2023 aimed to broaden
the scope of vulnerabilities and reflect current cybersecurity trends. However, the results did
not uniformly show enhanced performance across all model types, indicating variability in the
effectiveness of the added data. The variability suggests that while dataset augmentation has
potential, the quality of annotations—especially those provided by less experienced annotators like
students—plays a critical role in the success of model training.
These findings highlight the importance of not only applying advanced machine learning tech-
niques but also maintaining rigorous quality control in data annotation to optimize the performance
of models used in software vulnerability classification. This study lays the groundwork for further
research into dataset enhancements and the integration of LLMs with traditional models to refine
the processes of vulnerability assessment in the field of cybersecurity.


## DOWNLOAD MODELS
[Google Drive](https://drive.google.com/drive/folders/19IdDA7tUUfR40J7TbK5GOsJeGuIsgp3A?usp=sharing)

## REPOSITORY STRUCTURE
Folders:
- Dataset: Contains the combined dataset used in the research - the writer's annotated dataset and Okutan's et al original dataset.
- RQ1: The source code of all transformer-based models fine-tuned with the dataset above.
- RQ2: The source code of all transformer-based models fine-tuned with Okutan's et al original dataset.
- RQ3: The source code to extract n-grams.
